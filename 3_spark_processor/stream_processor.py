import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, udf, current_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, MapType, FloatType

# --- C·∫§U H√åNH ---
KAFKA_BOOTSTRAP_SERVERS = "localhost:9092"
TOPIC_NAME = "traffic-metadata"
# C·∫•u h√¨nh MongoDB
MONGO_URI = "mongodb://root:bigdataproject@localhost:27017/traffic_db.analysis_results?authSource=admin"

# Schema (Gi·ªØ nguy√™n)
kafka_schema = StructType([
    StructField("record_key", StringType(), True),
    StructField("camera_id", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("lmdb_info", StructType([
        StructField("lmdb_filepath", StringType(), True),
        StructField("frame_height", IntegerType(), True),
        StructField("frame_width", IntegerType(), True)
    ])),
    StructField("schema_version", StringType(), True)
])

ai_output_schema = StructType([
    StructField("counts", MapType(StringType(), IntegerType()), True),
    StructField("density", FloatType(), True),
    StructField("status", StringType(), True),
    StructField("traffic_status", StringType(), True),
    StructField("metrics_debug", MapType(StringType(), IntegerType()), True),
    StructField("error", StringType(), True)
])

# Import logic AI (C·∫ßn import sau khi addPyFile ·ªü th·ª±c t·∫ø, nh∆∞ng ·ªü local th√¨ import lu√¥n c≈©ng ƒë∆∞·ª£c)
# Tuy nhi√™n ƒë·ªÉ ch·∫Øc ch·∫Øn, ta gi·ªØ nguy√™n logic addPyFile
from udf_logic import process_image_logic

def create_spark_session():
    return SparkSession.builder \
        .appName("TrafficAnalysis_V2_Final_Mongo") \
        .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1,org.mongodb.spark:mongo-spark-connector_2.13:10.3.0") \
        .config("spark.sql.shuffle.partitions", "4") \
        .master("local[*]") \
        .getOrCreate()

def run_spark_job():
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")

    print("üöÄ ƒêang kh·ªüi ƒë·ªông Spark Streaming (Sink: MongoDB)...")

    # G·ª≠i file UDF cho Workers
    current_dir = os.path.dirname(os.path.abspath(__file__))
    udf_path = os.path.join(current_dir, "udf_logic.py")
    spark.sparkContext.addPyFile(udf_path)

    # ƒêƒÉng k√Ω UDF
    run_ai_udf = udf(process_image_logic, ai_output_schema)

    # ƒê·ªçc Kafka
    kafka_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
        .option("subscribe", TOPIC_NAME) \
        .option("startingOffsets", "earliest") \
        .option("maxOffsetsPerTrigger", 5) \
        .load()

    # Parse JSON
    parsed_df = kafka_df.select(
        from_json(col("value").cast("string"), kafka_schema).alias("data")
    ).select("data.*")

    # Ch·∫°y AI
    print("‚è≥ ƒêang x·ª≠ l√Ω AI...")
    ai_df = parsed_df.withColumn("ai_result", run_ai_udf(col("record_key")))

    # Flatten k·∫øt qu·∫£
    final_df = ai_df.select(
        col("record_key").alias("_id"),  # D√πng record_key l√†m ID ch√≠nh trong Mongo
        col("camera_id"),
        col("timestamp"),
        col("ai_result.traffic_status").alias("traffic_status"),
        col("ai_result.density").alias("density"),
        col("ai_result.counts").alias("vehicle_counts"),
        col("ai_result.metrics_debug").alias("debug_pixels"),
        col("ai_result.status").alias("proc_status"),
        current_timestamp().alias("processed_at")
    )

    # --- GHI V√ÄO MONGODB (Thay ƒë·ªïi ·ªü ƒë√¢y) ---
    query = final_df.writeStream \
        .format("mongodb") \
        .option("checkpointLocation", "/tmp/spark_checkpoint_mongo") \
        .option("forceDeleteTempCheckpointLocation", "true") \
        .option("spark.mongodb.connection.uri", MONGO_URI) \
        .option("spark.mongodb.database", "traffic_db") \
        .option("spark.mongodb.collection", "analysis_results") \
        .outputMode("append") \
        .start()

    print("‚úÖ Pipeline ƒëang ch·∫°y ng·∫ßm v√† ghi v√†o MongoDB...")
    query.awaitTermination()

if __name__ == "__main__":
    run_spark_job()